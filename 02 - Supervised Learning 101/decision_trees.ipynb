{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Trees\n",
        "\n\n",
        "Decision trees are flowchart-like structures composed by nodes, branches and leaves. A node is a test on a certain attribute, a branch represents the outcome of a test and finally, a leaf corresponds to the decision taken.\n",
        "\n",
        "This takes us to decision tree learning. The goal of this method is to create a model that predicts the outcome of a given variable based on several inputs.\n",
        "\n",
        "A simple example as seen in Tom Mitchell's \"Machine Learning\" is the following:\n",
        "### Example: Playing tennis\n",
        "![alternate text](https://www.cise.ufl.edu/~ddd/cap6635/Fall-97/Short-papers/Image3.gif)"
      ],
      "metadata": {
        "collapsed": false,
        "inputHidden": false,
        "outputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "------------------------------------------------------------------------------------------------------------------\n",
        "Pros:\n",
        "==\n",
        "\n",
        "* Simple to understand and to visualize.\n",
        "\n",
        "* Requires little data preparation.\n",
        "\n",
        "* Complexity of the algorithm is logarithmic in the number of points used to train the tree.\n",
        "\n",
        "Cons:\n",
        "==\n",
        "\n",
        "* Prone to overfitting. Aside from choosing the right values for the right parameters, it might require extra techniques, such as pruning to avoid this problem.\n",
        "\n",
        "* Decision trees might be unstable, with small variations in data resulting on very different trees.\n",
        "\nMore at: http://scikit-learn.org/stable/modules/tree.html#"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "------------------------------------------------------------------------------------------------------------------\n",
        "Mathematical Background:\n",
        "==\n",
        "\n",
        "There are plenty of implementations of Decision Trees. Scikit implements an optimised version of the **CART (Classification and Regression Trees)** algorithm. This algorithm works by constructing binary trees (a tree data structure in which each node has at most two children) using the feature and threshold that allow the tree to gain the biggest amount of information at each node.\n",
        "\n\n\n",
        "In this workshop we will use entropy as the measure of information gain and as the criterion in our decision tree classifier. Information gain is based on the concept of entropy from information theory.\n",
        "\n",
        "Information gain may be defined as:\n",
        "\n",
        "$$IG(T,a) = H(T) - H(T|a)$$\n",
        "\n",
        "which can be translated as *\"Information gain is given by the entropy of the parent node minus the weighted sum of entropy of the children nodes\"*. \n",
        "\n",
        "It's also important to define entropy:\n",
        "\n",
        "$$H(T) = -\\sum_{i=1}^j p_i log_2 p_i$$\n",
        "\n",
        "where $p_i$ represents the percentage of each class present in the child node that results in a split in the tree.\n",
        "\n",
        "A good way to understand the concept of entropy is by looking at the following graphic:\n",
        "\n",
        "![alternate text](https://upload.wikimedia.org/wikipedia/commons/thumb/2/22/Binary_entropy_plot.svg/320px-Binary_entropy_plot.svg.png?1490734279242)\n",
        "\n",
        "As we can see, entropy is a measure that has values between 0 and 1. When in an event with two possible outcomes, one has probability 0 and the other one 1, we have absolute certainty about the outcome and therefore, zero entropy on that decision. When we have two events that are equally probable, the uncertainty on the outcome is maximum and so is the entropy.\n",
        "\n",
        "Since the entropy of the parent node is always the same for a certain tree split, we may approach this problem by finding the decision that minimizes the sum of the entropy of the children nodes.\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree implementation"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Data visualization\n",
        "import pandas as pd\n",
        "\n",
        "iris_df = pd.read_csv(\"https://raw.githubusercontent.com/plotly/datasets/master/iris.csv\")\n",
        "print(\"Iris dataframe)\",iris_df.head(),sep=\"\\n\")\n",
        "print(\"\\n....\\n\")\n",
        "print(iris_df.tail(),sep=\"\\n\")\n",
        "print(\"\\n....\\n\")\n",
        "print(iris_df.info())\n",
        "print(\"\\n....\\n\")\n",
        "print(iris_df.describe())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iris dataframe)\n",
            "   SepalLength  SepalWidth  PetalLength  PetalWidth         Name\n",
            "0          5.1         3.5          1.4         0.2  Iris-setosa\n",
            "1          4.9         3.0          1.4         0.2  Iris-setosa\n",
            "2          4.7         3.2          1.3         0.2  Iris-setosa\n",
            "3          4.6         3.1          1.5         0.2  Iris-setosa\n",
            "4          5.0         3.6          1.4         0.2  Iris-setosa\n",
            "\n",
            "....\n",
            "\n",
            "     SepalLength  SepalWidth  PetalLength  PetalWidth            Name\n",
            "145          6.7         3.0          5.2         2.3  Iris-virginica\n",
            "146          6.3         2.5          5.0         1.9  Iris-virginica\n",
            "147          6.5         3.0          5.2         2.0  Iris-virginica\n",
            "148          6.2         3.4          5.4         2.3  Iris-virginica\n",
            "149          5.9         3.0          5.1         1.8  Iris-virginica\n",
            "\n",
            "....\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 150 entries, 0 to 149\n",
            "Data columns (total 5 columns):\n",
            "SepalLength    150 non-null float64\n",
            "SepalWidth     150 non-null float64\n",
            "PetalLength    150 non-null float64\n",
            "PetalWidth     150 non-null float64\n",
            "Name           150 non-null object\n",
            "dtypes: float64(4), object(1)\n",
            "memory usage: 5.9+ KB\n",
            "None\n",
            "\n",
            "....\n",
            "\n",
            "       SepalLength  SepalWidth  PetalLength  PetalWidth\n",
            "count   150.000000  150.000000   150.000000  150.000000\n",
            "mean      5.843333    3.054000     3.758667    1.198667\n",
            "std       0.828066    0.433594     1.764420    0.763161\n",
            "min       4.300000    2.000000     1.000000    0.100000\n",
            "25%       5.100000    2.800000     1.600000    0.300000\n",
            "50%       5.800000    3.000000     4.350000    1.300000\n",
            "75%       6.400000    3.300000     5.100000    1.800000\n",
            "max       7.900000    4.400000     6.900000    2.500000\n"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "------------------------------------------------------------------------------------------------------------------\n",
        "\nAs we can see, what we intend to use as label in our learning model (the names of the plants), is not a float and this will bring us problems later on when we try to train the model.. let's then start by encoding them to floats!"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding the labels to be usabel in our training model\n",
        "from sklearn import preprocessing\n",
        "\n",
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(iris_df[\"Name\"])\n",
        "y = le.transform(iris_df[\"Name\"])\n",
        "\nprint(\"Our column \\\"Name\\\" now looks like this:\\n {}\".format(y))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our column \"Name\" now looks like this:\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2]\n"
          ]
        }
      ],
      "execution_count": 2,
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "------------------------------------------------------------------------------------------------------------------\n",
        "\nNow that this problem is solved, we will split our dataset into two, one training set and on test set. This will allow us to check if our system is generalizing properly to unseen data and therefore, whether it is overfitting or not."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into a training and test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = iris_df.drop(\"Name\",1)\n",
        "\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "------------------------------------------------------------------------------------------------------------------\n",
        "We can now train our model!"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model with an decision tree classifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "dt = DecisionTreeClassifier(criterion='entropy', splitter='best', max_depth=3, min_samples_split=5, \n",
        "\t\t\t\t\t\t\tmin_samples_leaf=6, min_weight_fraction_leaf=0.0, max_features=None, \n",
        "\t\t\t\t\t\t\trandom_state=None, max_leaf_nodes=None, min_impurity_split=1e-07, class_weight=None, \n",
        "\t\t\t\t\t\t\tpresort=False)\n",
        "dt.fit(X_train,y_train)\n",
        "\n",
        "# Predicted labels for our test features - not needed, it's just to visualize it\n",
        "Y_pred = dt.predict(X_test)\n",
        "\n",
        "print(\"The predicted values are: \\n {}\".format(Y_pred))\n",
        "print(\"The values that should be obtained are: \\n {}\".format(y_test))\n",
        "\n",
        "score_train = dt.score(X_train,y_train)\n",
        "score_test = dt.score(X_test, y_test)\n",
        "\nprint(\"Our model has a score of {} for the train set and a score of {} for the test set\".format(score_train,score_test))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The predicted values are: \n",
            " [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1\n",
            " 0 0 0 2 1 1 0 0 1 1 2 1 2]\n",
            "The values that should be obtained are: \n",
            " [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1\n",
            " 0 0 0 2 1 1 0 0 1 2 2 1 2]\n",
            "Our model has a score of 0.95 for the train set and a score of 0.98 for the test set\n"
          ]
        }
      ],
      "execution_count": 4,
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "------------------------------------------------------------------------------------------------------------------\n",
        "Python allows us to visualize random trees. But since it requires some extra non-common libraries we will only show examples of obtainable trees. We provide the code nonetheless, so you can try it at home!"
      ],
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the decision tree\n",
        "\n",
        "\"\"\"\n",
        "import pydotplus \n",
        "from sklearn import tree\n",
        "\n",
        "features = X.columns[:4]\n",
        "\n",
        "dot_data = tree.export_graphviz(dt, out_file=None,\n",
        "\t\t\t\t\t\t\t\tfeature_names = features, class_names = ['setosa','versicolor','virginica'],\n",
        "\t\t\t\t\t\t\t\tfilled = True, rounded = True) \n",
        "graph = pydotplus.graph_from_dot_data(dot_data) \n",
        "graph.write_pdf(\"iris.pdf\")\"\"\""
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "text/plain": [
              "'\\nimport pydotplus \\nfrom sklearn import tree\\n\\nfeatures = X.columns[:4]\\n\\ndot_data = tree.export_graphviz(dt, out_file=None,\\n\\t\\t\\t\\t\\t\\t\\t\\tfeature_names = features, class_names = [\\'setosa\\',\\'versicolor\\',\\'virginica\\'],\\n\\t\\t\\t\\t\\t\\t\\t\\tfilled = True, rounded = True) \\ngraph = pydotplus.graph_from_dot_data(dot_data) \\ngraph.write_pdf(\"iris.pdf\")'"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 5,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "------------------------------------------------------------------------------------------------------------------\n",
        "Using a maximum depth of 1, minimum number of samples at a leaf node of 5, we obtain a **train score of 0.66 and a test score of 0.68**.\n",
        "\n",
        "![alternate text](http://i65.tinypic.com/21mi1s8.png)\n",
        "\n",
        "------------------------------------------------------------------------------------------------------------------\n",
        "But we can do better. Using a maximum depth of 2, our **train score improves to 0.96 and our test score to 0.98**.\n",
        "\n",
        "![alternate text](http://i63.tinypic.com/21jpspf.png)\n",
        "\n",
        "------------------------------------------------------------------------------------------------------------------\n",
        "What if we allow our tree to have a maximum depth of 3? We can finally get a **test score of 1, while having a train score of 0.98!**\n",
        "\n",
        "![alternate text](http://i65.tinypic.com/2nm3wba.png)\n",
        "\n",
        "------------------------------------------------------------------------------------------------------------------\n",
        "Let's now keep this maximum depth, but increase the minimum number of samples at a leaf node to 6. **Our train score decreased to 0.95 and our test score to 0.98.** This means that incresing this value made our system worse at learning the data..\n",
        "\n",
        "![alternate text](http://i68.tinypic.com/2qus01w.png)\n",
        "\n",
        "------------------------------------------------------------------------------------------------------------------\n",
        "And can we overfit such a small and simple dataset? Using a maximum depth of 6 and reducing our parameters min_samples_leaf and min_samples_split to 1 and 2, respectively, **we now have a train score of 1 and a test score of 0.98.** As we've seen in out last workshop, having a better accuracy in out training set than in out test set is a clear sign of overfitting!\n",
        "\n",
        "![alternate text](http://i63.tinypic.com/2njvwi0.png)\n",
        "\n",
        "And to visualize the space segmentation that this algorithm produces:\n",
        "\n![alternate text](http://scikit-learn.org/stable/_images/sphx_glr_plot_iris_0011.png)"
      ],
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "------------------------------------------------------------------------------------------------------------------\n",
        "**Key tips on implementing decision trees**\n",
        "==\n",
        "\n",
        "* **Use the right ratio of samples to number of features** - Decision trees have the tendency to overfit when trained with data that uses a large number of features. It's a good idea to perform dimensionality reduction beforehand.\n",
        "\n",
        "* **Start with maximum deepness of 3** - Start with this value and change it accordingly to your results. It's an important aspect to prevent overfitting.\n",
        "\n",
        "* **Start with a value of 5 for the minimum number of samples on min_samples_split and min_samples_leaf** - This parameters are used to control the number of samples at a leaf node. As we've seen in our examples, lower values will make your tree overfit (you allow your tree to have leafs with a smaller number of samples -> tree will generalize poorly to unseen data), while higher values will prevent your tree to learn the data (if each leaf has too have too many samples -> your tree will be to general and will not describe your data).\n",
        "\nmore tips at: http://scikit-learn.org/stable/modules/tree.html#tips-on-practical-use"
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "version": "3.4.5",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}